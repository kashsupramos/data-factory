# ğŸ­ Data Factory - Automated Training Dataset Generator

> **"Paste URL â†’ Configure â†’ Run â†’ Get Training Data"**

A controlled pipeline for generating high-quality, versioned training datasets from web content. Built for teams who need traceable, auditable data for LLM fine-tuning.

## âœ¨ Features

- ğŸ•·ï¸ **Smart Web Crawling** - Configurable page limits and delays
- ğŸ§¹ **Intelligent Cleaning** - Removes boilerplate while preserving critical data (prices, measurements)
- âœ‚ï¸ **Context-Aware Slicing** - Breaks content into optimal chunks without splitting important information
- ğŸ¤– **AI-Powered Q&A Generation** - Uses Groq API to create training pairs
- ğŸ“¦ **Versioned Outputs** - Every run creates a new timestamped dataset (never overwrites)
- ğŸ” **Fully Traceable** - Track URLs, configs, and stats for every dataset

## ğŸš€ Quick Start

### Prerequisites
- Python 3.9+
- Groq API key ([get one free at groq.com](https://console.groq.com/))

### Installation

1. **Clone the repo:**
```bash
git clone https://github.com/yourusername/data-factory.git
cd data-factory
```

2. **Create virtual environment:**
```bash
python -m venv venv

# On Windows:
venv\Scripts\activate

# On Mac/Linux:
source venv/bin/activate
```

3. **Install dependencies:**
```bash
pip install -r requirements.txt
```

4. **Create `.env` file in project root:**
```env
GROQ_API_KEY=your_api_key_here
```

### Run the Pipeline

**Option 1: GUI (Recommended for beginners)**
```bash
python gui.py
```
Then open your browser to http://127.0.0.1:7860 and use the web interface!

**Option 2: Command Line**
```bash
cd Claudedatasets
python pipeline.py https://example.com --max-pages 50
```

**That's it!** Your dataset will be in `AllDatasets/runs/run_YYYY-MM-DD_HH-MM-SS_XXXXXX/`

## ğŸ“– Usage Examples

### Basic crawl (default 100 pages):
```bash
python pipeline.py https://example.com
```

### Crawl with custom limits:
```bash
python pipeline.py https://example.com --max-pages 50 --delay 2.0
```

### Interactive mode (prompts for URL):
```bash
python pipeline.py
```

## ğŸ“ Output Structure

Each run creates a timestamped folder with 4 files:

```
AllDatasets/runs/run_2025-12-23_08-05-26_983378/
â”œâ”€â”€ crawl_raw.jsonl      # Raw scraped data
â”œâ”€â”€ crawl_clean.jsonl    # Cleaned documents
â”œâ”€â”€ crawl_sliced.jsonl   # Sliced into training blocks
â””â”€â”€ qa_training.jsonl    # Question-answer pairs (ready for training!)
```

### Output Format

**qa_training.jsonl** - Ready for LLM fine-tuning:
```json
{"source_url": "https://example.com", "page_type": "product", "question": "What is...", "answer": "..."}
{"source_url": "https://example.com", "page_type": "faq", "question": "How to...", "answer": "..."}
```

## ğŸ› ï¸ Current Features

- âœ… **Beautiful Web GUI** (no command line needed!)
- âœ… Automated 4-step pipeline (Crawl â†’ Clean â†’ Slice â†’ Generate Q&A)
- âœ… Real-time progress tracking
- âœ… One-click downloads
- âœ… Groq API integration (llama-3.1-8b-instant)
- âœ… Configurable crawl limits and delays
- âœ… Smart content slicing (preserves prices, measurements, percentages)
- âœ… Versioned outputs (never overwrites previous runs)
- âœ… Windows/Mac/Linux compatible
- âœ… UTF-8 emoji support

## ğŸš§ Roadmap (Help Wanted!)

We're actively looking for contributors! Here's what we're planning:

### High Priority
- [ ] **GUI Interface** (Gradio/Streamlit) - drag & drop URLs, configure settings
- [ ] **Multiple LLM Providers** (OpenAI, Claude, local models via LM Studio)
- [ ] **Strictness Modes** (Conservative/Balanced/Aggressive Q&A generation)

### Medium Priority
- [ ] **robots.txt Compliance** - respect crawl rules automatically
- [ ] **Crawl Depth Controls** - limit how deep to follow links
- [ ] **Domain-Specific Rules** - custom filtering per project
- [ ] **Audit Dashboard** - review stats, sample outputs before export

### Nice to Have
- [ ] **Batch URL Processing** - upload CSV of URLs
- [ ] **Resume Failed Runs** - continue from where it crashed
- [ ] **Export Formats** - CSV, Parquet, HuggingFace datasets
- [ ] **Quality Metrics** - automatic scoring of Q&A pairs

## ğŸ¤ Contributing

We'd love your help! Here's how to contribute:

1. **Fork the repo**
2. **Create a feature branch** (`git checkout -b feature/amazing-feature`)
3. **Make your changes**
4. **Test it works** (run the pipeline on a test URL)
5. **Commit** (`git commit -m 'Add amazing feature'`)
6. **Push** (`git push origin feature/amazing-feature`)
7. **Open a Pull Request**

### Good First Issues
- Add support for more LLM providers
- Improve error handling
- Add unit tests
- Create example notebooks
- Improve documentation

## ğŸ“ Project Structure

```
data-factory/
â”œâ”€â”€ Claudedatasets/              # Main pipeline scripts
â”‚   â”œâ”€â”€ pipeline.py              # Master orchestrator
â”‚   â”œâ”€â”€ crawling.py              # Web scraper
â”‚   â”œâ”€â”€ cleancrawling.py         # Data cleaner
â”‚   â”œâ”€â”€ slicingdata.py           # Content slicer
â”‚   â””â”€â”€ generate_qa_dataset.py   # Q&A generator
â”œâ”€â”€ AllDatasets/
â”‚   â””â”€â”€ runs/                    # Output datasets (timestamped)
â”œâ”€â”€ .env                         # API keys (not in repo - you create this)
â”œâ”€â”€ .gitignore                   # Files to ignore
â”œâ”€â”€ requirements.txt             # Python dependencies
â””â”€â”€ README.md                    # This file!
```

## ğŸ”§ Configuration

### Environment Variables (.env)
```env
GROQ_API_KEY=your_groq_api_key_here
```

### Pipeline Options
- `--max-pages`: Maximum pages to crawl (default: 100)
- `--delay`: Delay between requests in seconds (default: 1.0)

## ğŸ› Troubleshooting

### "ModuleNotFoundError: No module named 'bs4'"
Make sure you activated the virtual environment and installed dependencies:
```bash
venv\Scripts\activate  # Windows
pip install -r requirements.txt
```

### "GROQ_API_KEY not found"
Create a `.env` file in the project root with your API key:
```env
GROQ_API_KEY=your_key_here
```

### Unicode/Emoji Errors on Windows
This should be fixed! But if you see encoding errors, the scripts auto-detect Windows and set UTF-8.

## ğŸ“„ License

MIT License - feel free to use for your projects!

## ğŸ™ Acknowledgments

Built with â¤ï¸ for the open-source ML community during the holidays!

Special thanks to:
- Groq for fast inference
- BeautifulSoup for web scraping
- The Python community

---

**Questions?** Open an issue or reach out to the team!

**Want to chat?** Join our discussions tab!

